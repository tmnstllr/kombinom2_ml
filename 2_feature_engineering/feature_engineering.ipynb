{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv = \"combined_data_MP_NE_dT_Coord_cC.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.isna()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# calculate and add quarter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['quarter'] = (df['month'] - 1) // 3 + 1\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"quarter\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# add number of day"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weekday_mapping = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday/Holiday': 6}\n",
    "df['weekday_number'] = df['weekday'].map(weekday_mapping)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['weekday_number'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# add isWeekend"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the new column 'isWeekend' using numpy.where()\n",
    "df['isWeekend'] = np.where(df['weekday'].isin(['Saturday', 'Sunday/Holiday']), True, False)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contains_nan = df.isnull().any()\n",
    "contains_nan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(csv.split(sep='.')[0] + '_fE' + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assign quarter based on month\n",
    "df['quarter'] = (df['month'] - 1) // 3 + 1\n",
    "\n",
    "# Add number of day in the week\n",
    "weekday_mapping = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2,\n",
    "                   'Thursday': 3, 'Friday': 4, 'Saturday': 5,\n",
    "                   'Sunday/Holiday': 6}\n",
    "df['weekday_number'] = df['weekday'].map(weekday_mapping)\n",
    "\n",
    "# Add isWeekend flag\n",
    "df['isWeekend'] = np.where(df['weekday'].isin(\n",
    "    ['Saturday', 'Sunday/Holiday']), True, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add OSM data as features (like counts for schools etc.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv = \"combined_data_MP_NE_mappedUCOtoMTC_Coord_dT_fE_mergedClusterInside_cC_start.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.count_corrected.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_osm = \"osm_data_cleaned.csv\"\n",
    "df_osm = pd.read_csv(csv_osm)\n",
    "df_osm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_osm.clusterID.unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xlsx = \"mappingInsideStudyArea.xlsx\"\n",
    "df_xlsx = pd.read_excel(xlsx)\n",
    "df_xlsx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from df_api\n",
    "mapping_dict_inside = df_xlsx.set_index('search_id')['id'].to_dict()\n",
    "mapping_dict_inside"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply the mapping to the column in df, keeping original value if no match\n",
    "df_osm['clusterID'] = df_osm['clusterID'].apply(lambda x: mapping_dict_inside.get(x, x))\n",
    "df_osm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply the mapping to the column in df, keeping original value if no match\n",
    "df_osm['cluster'] = df_osm['cluster'].apply(lambda x: x.split(' (')[0])\n",
    "df_osm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_osm.cluster.unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "one way"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_osm.drop(columns='cluster', inplace=True)\n",
    "df_osm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "another way"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_osm = df_osm.groupby(['clusterID']).agg({\n",
    "    'kindergartens_count': 'sum',\n",
    "    'schools_count': 'sum',\n",
    "    'sportsCentres_count': 'sum',\n",
    "    'publicTransportStops_count': 'sum',\n",
    "    'supermarkets_count': 'sum'\n",
    "})\n",
    "df_osm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged = df.merge(df_osm, how='left', left_on='startClusterID', right_on='clusterID')\n",
    "df_merged.fillna(0, inplace=True)\n",
    "df_merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_merged.drop(columns='clusterID', inplace=True)\n",
    "df_merged['kindergartens_count'] = df_merged['kindergartens_count'].astype(int)\n",
    "df_merged['schools_count'] = df_merged['schools_count'].astype(int)\n",
    "df_merged['sportsCentres_count'] = df_merged['sportsCentres_count'].astype(int)\n",
    "df_merged['publicTransportStops_count'] = df_merged['publicTransportStops_count'].astype(int)\n",
    "df_merged['supermarkets_count'] = df_merged['supermarkets_count'].astype(int)\n",
    "df_merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_merged.to_csv(csv.split(sep='.')[0] + '_OSM' + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clip count & Distance metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv = \"combined_data_MP_NE_mappedUCOtoMTC_Coord_dT_fE_mergedClusterInside_cC_start_OSM.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sort the data by 'year', 'quarter', and 'month'\n",
    "df = df.sort_values(by=['year', 'quarter', 'month'])\n",
    "\n",
    "# Add lag features for the last 3 months\n",
    "df['count_lag1'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=1)\n",
    "df['count_lag2'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=2)\n",
    "df['count_lag3'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=3)\n",
    "\n",
    "# Calculate the sum of 'count_corrected' of the previous month for each 'startClusterID'\n",
    "df['sum_of_count_lag1'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=1).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag2'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=2).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag3'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=3).groupby(df['startClusterID']).cumsum()\n",
    "\n",
    "# Calculate the metrics for 'sum_of_count_previous_month' for each 'startClusterID'\n",
    "metrics = df.groupby(['startClusterID'])['sum_of_count_lag1'].agg(['std', 'median', ('25th_percentile', lambda x: x.quantile(0.25)), ('75th_percentile', lambda x: x.quantile(0.75))]).reset_index()\n",
    "\n",
    "# Rename the columns to include the metric names\n",
    "metrics.columns = ['startClusterID', 'std_of_previous_month', 'median_of_previous_month', 'quantile_25_of_previous_month', 'quantile_75_of_previous_month']\n",
    "\n",
    "# Merge the metrics DataFrame with the original DataFrame based on 'startClusterID'\n",
    "df = df.merge(metrics, on=['startClusterID'], how='left')\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sort the data by 'year', 'quarter', and 'month'\n",
    "df = df.sort_values(by=['year', 'quarter', 'month'])\n",
    "\n",
    "# Add lag features for the last 3 months\n",
    "df['count_lag1'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected'].shift(periods=1)\n",
    "df['count_lag2'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected'].shift(periods=2)\n",
    "df['count_lag3'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected'].shift(periods=3)\n",
    "\n",
    "# Calculate the sum of 'count_corrected' of the previous month for each 'startClusterID'\n",
    "df['sum_of_count_lag1'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected'].shift(periods=1).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag2'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected'].shift(periods=2).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag3'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected'].shift(periods=3).groupby(df['startClusterID']).cumsum()\n",
    "\n",
    "\n",
    "# Calculate the metrics for 'sum_of_count_previous_month' for each 'startClusterID'\n",
    "metrics = df.groupby(['startClusterID', 'endClusterID'])['sum_of_count_lag1'].agg(['std', 'median', ('25th_percentile', lambda x: x.quantile(0.25)), ('75th_percentile', lambda x: x.quantile(0.75))]).reset_index()\n",
    "\n",
    "# Rename the columns to include the metric names\n",
    "metrics.columns = ['startClusterID', 'endClusterID', 'std_of_previous_month', 'median_of_previous_month', 'quantile_25_of_previous_month', 'quantile_75_of_previous_month']\n",
    "\n",
    "# Merge the metrics DataFrame with the original DataFrame based on 'startClusterID'\n",
    "df = df.merge(metrics, on=['startClusterID', 'endClusterID'], how='left')\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 (for the first month of each startClusterID)\n",
    "df.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sort the data by 'year', 'quarter', and 'month'\n",
    "df = df.sort_values(by=['year', 'quarter', 'month'])\n",
    "\n",
    "# Add lag features of 'count_corrected' for the last 3 months\n",
    "df['count_lag1'] = df.groupby(['startClusterID'])['count_corrected']\\\n",
    "    .shift(periods=1)\n",
    "df['count_lag2'] = df.groupby(['startClusterID'])['count_corrected']\\\n",
    "    .shift(periods=2)\n",
    "df['count_lag3'] = df.groupby(['startClusterID'])['count_corrected']\\\n",
    "    .shift(periods=3)\n",
    "\n",
    "# Add lag features of the sum of 'count_corrected' for the last 3 months\n",
    "df['sum_of_count_lag1'] = df.groupby(['startClusterID'])['count_corrected']\\\n",
    "    .shift(periods=1).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag2'] = df.groupby(['startClusterID'])['count_corrected']\\\n",
    "    .shift(periods=2).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag3'] = df.groupby(['startClusterID'])['count_corrected']\\\n",
    "    .shift(periods=3).groupby(df['startClusterID']).cumsum()\n",
    "\n",
    "# Calculate the metrics for 'sum_of_count_previous_month' for each 'startClusterID'\n",
    "metrics = df.groupby(['startClusterID'])['sum_of_count_lag1'].agg([\n",
    "    'std', 'median', ('25th_percentile', lambda x: x.quantile(0.25)),\n",
    "    ('75th_percentile', lambda x: x.quantile(0.75))]).reset_index()\n",
    "# Rename the columns to include the metric names\n",
    "metrics.columns = ['startClusterID', 'std_of_previous_month',\n",
    "                   'median_of_previous_month', 'quantile_25_of_previous_month',\n",
    "                   'quantile_75_of_previous_month']\n",
    "# Merge the metrics DataFrame with the original DataFrame based on 'startClusterID'\n",
    "df = df.merge(metrics, on=['startClusterID'], how='left')\n",
    "\n",
    "# Fill NaN values with 0 (relevant for lag features)\n",
    "df.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sort the data by 'year', 'quarter', and 'month'\n",
    "df = df.sort_values(by=['year', 'quarter', 'month'])\n",
    "\n",
    "# Add lag features of 'count_corrected' for the last 3 months\n",
    "df['count_lag1'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected']\\\n",
    "    .shift(periods=1)\n",
    "df['count_lag2'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected']\\\n",
    "    .shift(periods=2)\n",
    "df['count_lag3'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected']\\\n",
    "    .shift(periods=3)\n",
    "\n",
    "# Calculate the sum of 'count_corrected' of the previous month for each 'startClusterID'\n",
    "df['sum_of_count_lag1'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected']\\\n",
    "    .shift(periods=1).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag2'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected']\\\n",
    "    .shift(periods=2).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag3'] = df.groupby(['startClusterID', 'endClusterID'])['count_corrected']\\\n",
    "    .shift(periods=3).groupby(df['startClusterID']).cumsum()\n",
    "\n",
    "# Calculate the metrics for 'sum_of_count_previous_month' for each 'startClusterID'\n",
    "metrics = df.groupby(['startClusterID', 'endClusterID'])['sum_of_count_lag1'].agg([\n",
    "    'std',\n",
    "    'median',\n",
    "    ('25th_percentile', lambda x: x.quantile(0.25)),\n",
    "    ('75th_percentile', lambda x: x.quantile(0.75))\n",
    "]).reset_index()\n",
    "# Rename the columns to include the metric names\n",
    "metrics.columns = ['startClusterID', 'endClusterID', 'std_of_previous_month',\n",
    "                   'median_of_previous_month', 'quantile_25_of_previous_month',\n",
    "                   'quantile_75_of_previous_month']\n",
    "# Merge the metrics DataFrame with the original DataFrame based on 'startClusterID'\n",
    "df = df.merge(metrics, on=['startClusterID', 'endClusterID'], how='left')\n",
    "\n",
    "# Fill NaN values with 0 (relevant for lag features)\n",
    "df.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(csv.split(sep='.')[0] + '_metrics' + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "random stuff"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['distance'], df['count_corrected'], alpha=0.5)\n",
    "\n",
    "plt.xlabel('Distance', fontsize=14)\n",
    "plt.ylabel('Count Corrected', fontsize=14)\n",
    "plt.title('Relationship between Count Corrected and Distance', fontsize=16)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['distance'].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the 'count_corrected' and 'count_of_previous_month' columns\n",
    "\n",
    "# Linear Regression\n",
    "x = df['distance']\n",
    "y = df['count_corrected']\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r_value**2\n",
    "\n",
    "# Correlation\n",
    "correlation = x.corr(y)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(\"Slope:\", slope)\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n",
    "print(\"\\nCorrelation:\")\n",
    "print(\"Correlation coefficient:\", correlation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sort the data by 'year', 'quarter', and 'month'\n",
    "df = df.sort_values(by=['year', 'quarter', 'month'])\n",
    "\n",
    "# Add lag features for the last 3 months\n",
    "df['count_lag1'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=1)\n",
    "df['count_lag2'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=2)\n",
    "df['count_lag3'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=3)\n",
    "\n",
    "# Calculate the sum of 'count_corrected' of the previous month for each 'startClusterID'\n",
    "df['sum_of_count_lag1'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=1).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag2'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=2).groupby(df['startClusterID']).cumsum()\n",
    "df['sum_of_count_lag3'] = df.groupby(['startClusterID'])['count_corrected'].shift(periods=3).groupby(df['startClusterID']).cumsum()\n",
    "\n",
    "\n",
    "# Calculate the metrics for 'sum_of_count_previous_month' for each 'startClusterID'\n",
    "metrics = df.groupby(['startClusterID'])['sum_of_count_lag1'].agg(['std', 'median', ('25th_percentile', lambda x: x.quantile(0.25)), ('75th_percentile', lambda x: x.quantile(0.75))]).reset_index()\n",
    "\n",
    "# Rename the columns to include the metric names\n",
    "metrics.columns = ['startClusterID', 'std_of_previous_month', 'median_of_previous_month', 'quantile_25_of_previous_month', 'quantile_75_of_previous_month']\n",
    "\n",
    "# Merge the metrics DataFrame with the original DataFrame based on 'startClusterID'\n",
    "df = df.merge(metrics, on='startClusterID', how='left')\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 (for the first month of each startClusterID)\n",
    "df.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['count_lag1'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Assuming 'df' is your DataFrame with the 'count_corrected' and 'count_of_previous_month' columns\n",
    "\n",
    "# Linear Regression\n",
    "x = df['count_corrected']\n",
    "y = df['count_lag1']\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r_value**2\n",
    "\n",
    "# Correlation\n",
    "correlation = x.corr(y)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(\"Slope:\", slope)\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n",
    "print(\"\\nCorrelation:\")\n",
    "print(\"Correlation coefficient:\", correlation)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, alpha=0.5)\n",
    "\n",
    "plt.xlabel(f'{x.name}', fontsize=14)\n",
    "plt.ylabel(f'{y.name}', fontsize=14)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_id = df.groupby(['startClusterID', 'startClusterName'])['count_corrected'].sum().reset_index()\n",
    "df_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the sum of count_corrected for each group\n",
    "group_sums = df_id.groupby('startClusterID')['count_corrected'].sum()\n",
    "\n",
    "# Sort the groups by sum in descending order\n",
    "sorted_groups = group_sums.sort_values(ascending=True)\n",
    "\n",
    "# Create a dictionary to map old startClusterID values to new ones\n",
    "cluster_id_mapping = {old_id: new_id for new_id, old_id in enumerate(sorted_groups.index)}\n",
    "\n",
    "# Apply the mapping to create a new column with updated startClusterID values\n",
    "df_id['new_startClusterID'] = df_id['startClusterID'].map(cluster_id_mapping)\n",
    "\n",
    "df_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace the startClusterID values in the original df using the mapping\n",
    "df['startClusterID'] = df['startClusterID'].map(cluster_id_mapping)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.drop(columns=['startClusterName', 'startClusterZip', 'count_lag2', 'count_lag3','sum_of_count_lag1', 'sum_of_count_lag2', 'sum_of_count_lag3'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(csv.split(sep='.')[0] + '_metrics' + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# unrelated - combine routes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv = \"combined_data_MP_NE_with_coordinatessumCountPerRoutes.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"df['StartLon'] = df['StartLon'].astype(str)\n",
    "df['StartLat'] = df['StartLat'].astype(str)\n",
    "df['EndLon'] = df['EndLon'].astype(str)\n",
    "df['EndLat'] = df['EndLat'].astype(str)\"\"\"\n",
    "\n",
    "# Sort the values in each row to treat combinations as equivalent\n",
    "df[['StartName', 'EndName']] = pd.DataFrame(np.sort(df[['StartName', 'EndName']], axis=1), index=df.index)\n",
    "\n",
    "# Group by the sorted combinations and aggregate other columns\n",
    "df_grouped = df.groupby(['StartName', 'EndName'], as_index=False).agg({\n",
    "    'Count': 'sum'\n",
    "})\n",
    "df_grouped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.Count.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_grouped.Count.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_grouped.to_csv(csv.split('.')[0] + '_sumPerRoutesBi.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv = \"combined_data_MP_NE_with_coordinatessumCountPerRoutes_sumPerRoutesBi.csv\"\n",
    "df = pd.read_csv(csv)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_start = df.groupby(['StartName'], as_index=False).agg({\n",
    "    'Count': 'sum'\n",
    "})\n",
    "df_start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_end = df.groupby(['EndName'], as_index=False).agg({\n",
    "    'Count': 'sum'\n",
    "})\n",
    "df_end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum = pd.merge(df_start, df_end, how='outer', left_on='StartName', right_on='EndName')\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum.fillna(0, inplace=True)\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum['Count'] = df_sum['Count_x'] + df_sum['Count_y']\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum.drop(columns=['Count_x', 'Count_y'], inplace=True)\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum.loc[df_sum['StartName'] == 0, 'StartName'] = df_sum['EndName']\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum = df_sum.groupby('StartName')['Count'].sum().reset_index()\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum.Count.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_shapes = \"desd-4-landkreis-hildesheim-1663168323029-shapes.csv\"\n",
    "df_shapes = pd.read_csv(csv_shapes)\n",
    "df_shapes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum.rename(columns={'StartName': 'name'}, inplace=True)\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum = df_sum.merge(df_shapes, on='name', how='left')\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the 'Count' column\n",
    "df_sum['Count_normalized'] = scaler.fit_transform(df_sum[['Count']])\n",
    "df_sum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_sum.to_csv(csv.split('.')[0] + '_sumPerCluster.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
